name: Daily Real Estate Scraping

on:
  schedule:
    - cron: '0 22 * * *'  # 10:00 PM UTC = 12:00 AM Egypt Time (Ù…Ù†ØªØµÙ Ø§Ù„Ù„ÙŠÙ„)
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Run in test mode (2 pages only)'
        required: false
        default: 'no'
        type: choice
        options:
          - yes
          - no

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.GH_TOKEN }}
        fetch-depth: 0
    
    - name: Set timezone info
      run: |
        echo "ðŸŒ Timezone Information:"
        echo "Scheduled time: 22:00 UTC"
        echo "Egypt time (UTC+2): 00:00 (midnight)"
        date
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas numpy requests beautifulsoup4 lxml
        
    - name: Backup current data
      run: |
        if [ -f "Final1.csv" ]; then
          BACKUP_NAME="Final1_backup_$(date +%Y%m%d_%H%M%S).csv"
          cp Final1.csv "$BACKUP_NAME"
          echo "âœ… Backup created: $BACKUP_NAME"
        fi
        
    - name: Run scraping script
      env:
        SCRAPE_TIME: "midnight"
      run: |
        echo "â° Running at midnight Egypt time (22:00 UTC)"
        if [ "${{ github.event.inputs.test_mode }}" = "yes" ]; then
          echo "ðŸ”§ Running in TEST mode (2 pages only)"
          python -c "
          import sys
          sys.path.append('.')
          try:
              import scrape_data
              print('Test import successful')
          except Exception as e:
              print(f'Error: {e}')
          "
        else
          echo "ðŸš€ Running in PRODUCTION mode"
          python scrape_data.py
        fi
        
    - name: Check for data loss
      run: |
        if [ -f "scraping_metadata.txt" ]; then
          echo "ðŸ“Š Scraping Results:"
          cat scraping_metadata.txt
          
          # ØªØ­Ù‚Ù‚ Ù…Ù† ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
          if grep -q "Net change: -[0-9]*[1-9]" scraping_metadata.txt; then
            echo "âš ï¸ WARNING: Data loss detected!"
            LATEST_BACKUP=$(ls -t Final1_backup_*.csv | head -1)
            if [ -f "$LATEST_BACKUP" ]; then
              echo "Restoring from backup: $LATEST_BACKUP"
              cp "$LATEST_BACKUP" Final1.csv
            fi
          fi
        fi
        
    - name: Configure Git
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        
    - name: Commit and push changes
      env:
        GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
      run: |
        # ØªÙØ­Øµ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª
        if git diff --name-only | grep -q "Final1.csv\|scraping_metadata.txt"; then
          echo "ðŸ“ Changes detected"
          
          git add Final1.csv scraping_metadata.txt
          
          # Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† metadata
          if [ -f "scraping_metadata.txt" ]; then
            NEW_PROPS=$(grep "New properties added:" scraping_metadata.txt | awk '{print $NF}' || echo "0")
            TOTAL_PROPS=$(grep "Total properties:" scraping_metadata.txt | awk '{print $NF}' || echo "0")
            COMMIT_MSG="ðŸŒ™ Midnight Update: Added $NEW_PROPS properties (Total: $TOTAL_PROPS) [$(date +'%Y-%m-%d %H:%M EET')]"
          else
            COMMIT_MSG="ðŸŒ™ Midnight Update: Data refresh [$(date +'%Y-%m-%d %H:%M EET')]"
          fi
          
          git commit -m "$COMMIT_MSG"
          
          # pull Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª
          git pull origin main --rebase --strategy-option=theirs || true
          
          # push
          if git push origin main; then
            echo "âœ… Successfully pushed changes"
          else
            echo "âŒ Failed to push changes"
            exit 1
          fi
        else
          echo "âœ… No changes to commit"
        fi
        
    - name: Cleanup old backups
      run: |
        # Ø­Ø°Ù Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (Ø§Ø­ØªÙØ¸ Ø¨Ù€ 7 Ø£Ø­Ø¯Ø« ÙÙ‚Ø·)
        BACKUP_COUNT=$(ls -1 Final1_backup_*.csv 2>/dev/null | wc -l)
        if [ $BACKUP_COUNT -gt 7 ]; then
          ls -t Final1_backup_*.csv | tail -n +8 | xargs rm -f
          echo "ðŸ§¹ Cleaned old backups, keeping 7 latest"
        else
          echo "ðŸ“¦ Keeping all backups ($BACKUP_COUNT files)"
        fi
        
    - name: Create summary
      run: |
        echo "### ðŸŒ™ Midnight Scraping Completed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Scheduled:** 22:00 UTC (00:00 Egypt Time)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "scraping_metadata.txt" ]; then
          echo "**Results:**" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat scraping_metadata.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        else
          echo "**Status:** No new data collected or scraping failed" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Next run:** Tomorrow at 22:00 UTC (00:00 Egypt Time)" >> $GITHUB_STEP_SUMMARY
