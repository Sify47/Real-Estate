# scrape_data.py
import pandas as pd
import requests
import numpy as np
from bs4 import BeautifulSoup
import time
from datetime import datetime
import os
import sys


def scrape_bayut_page(page_url):
    """Ø¯Ø§Ù„Ø© Ù„Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø©"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    try:
        response = requests.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© {page_url}: {e}")
        return []

    def text_or_none(selector, parent):
        el = parent.select_one(selector)
        return el.get_text(strip=True) if el else None

    # select li cards inside the ul (avoid iterating the ul itself)
    property_cards = soup.select("ul._172b35d1 li")
    properties = []

    for card in property_cards:
        try:
            a = card.select_one("a._8969fafd")
            link = (
                f"https://www.bayut.eg{a.get('href')}" if a and a.get("href") else None
            )

            price = text_or_none(
                "h4.afdad5da._71366de7 span.eff033a6", card
            ) or text_or_none("span.eff033a6", card)
            title = text_or_none("h2._34c51035", card)

            spans = card.select("span._3002c6fb")
            type_ = spans[0].get_text(strip=True) if len(spans) > 0 else None
            bedrooms = spans[1].get_text(strip=True) if len(spans) > 1 else None
            bathrooms = spans[2].get_text(strip=True) if len(spans) > 2 else None

            location = text_or_none("h3._51c6b1ca", card)
            d = text_or_none("span.fd7ade6e", card)

            area_raw = text_or_none("h4._60820635._07b5f28e", card) or text_or_none(
                "h4", card
            )
            area = area_raw[:-6] if area_raw and len(area_raw) > 6 else area_raw

            properties.append(
                {
                    "PropertyType": type_,
                    "Link": link,
                    "Title": title,
                    "Price": price,
                    "Location": location,
                    "Area": area,
                    "Bedrooms": bedrooms,
                    "Bathrooms": bathrooms,
                    "Down_Payment": d,
                }
            )
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒØ§Ø±Ø¯: {e}")
            continue

    return properties


def scrape_all_pages(base_url, max_pages=20):
    """Ø¯Ø§Ù„Ø© Ù„Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª"""
    all_properties = []

    # Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰
    print(f"Ø¬Ø§Ø±ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØµÙØ­Ø© 1...")
    page1_properties = scrape_bayut_page(base_url)
    all_properties.extend(page1_properties)
    print(f"ØªÙ… Ø¬Ù…Ø¹ {len(page1_properties)} Ø¹Ù‚Ø§Ø± Ù…Ù† Ø§Ù„ØµÙØ­Ø© 1")

    # Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©
    for page_num in range(2, max_pages + 1):
        page_url = f"{base_url.rstrip('/')}/page-{page_num}/"
        print(f"Ø¬Ø§Ø±ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page_num}...")

        properties = scrape_bayut_page(page_url)

        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¹Ù‚Ø§Ø±Ø§Øª ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„ØµÙØ­Ø©ØŒ ØªÙˆÙ‚Ù
        if not properties:
            print(f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù‚Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page_num}. Ø§Ù„ØªÙˆÙ‚Ù...")
            break

        all_properties.extend(properties)
        print(f"ØªÙ… Ø¬Ù…Ø¹ {len(properties)} Ø¹Ù‚Ø§Ø± Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page_num}")

        # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø­Ø¸Ø± IP
        time.sleep(1)

    return all_properties


def clean_data_step1(df_clean):
    """Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù† Ø§Ù„ØªÙ†Ø¸ÙŠÙ"""
    # Ensure 'Location' exists
    if "Location" not in df_clean.columns:
        return df_clean

    # Split Location into parts and concatenate (drop original Location to avoid duplication)
    df_split = df_clean["Location"].str.split(",", expand=True).add_prefix("Location_")
    df_clean = pd.concat([df_clean.drop(columns=["Location"]), df_split], axis=1)

    # Drop Location_2 if present (some rows may not have 3 parts)
    if "Location_2" in df_clean.columns:
        df_clean = df_clean.drop(columns=["Location_2"])

    # Rename parts to meaningful names
    if "Location_1" in df_clean.columns:
        df_clean = df_clean.rename(columns={"Location_1": "State"})
    else:
        df_clean["State"] = np.nan

    if "Location_0" in df_clean.columns:
        df_clean = df_clean.rename(columns={"Location_0": "Location"})
    else:
        df_clean["Location"] = np.nan

    # Normalize text values safely on the df_clean dataframe
    if "State" in df_clean.columns:
        mask_state = df_clean["State"].notna()
        df_clean.loc[mask_state, "State"] = df_clean.loc[
            mask_state, "State"
        ].str.replace("Saba Pasha", "Saba Basha", case=False, regex=False)
        df_clean.loc[mask_state, "State"] = df_clean.loc[
            mask_state, "State"
        ].str.replace("Borg al-Arab", "Borg El Arab", case=False, regex=False)

    if "Location" in df_clean.columns:
        mask_loc = df_clean["Location"].notna()
        df_clean.loc[mask_loc, "Location"] = df_clean.loc[
            mask_loc, "Location"
        ].str.replace("Smoha", "Smouha", case=False, regex=False)

    return df_clean


# ...existing code...
def clean_data_step2(df_clean):
    """Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù…Ù† Ø§Ù„ØªÙ†Ø¸ÙŠÙ (fixed .str accessor errors)"""
    # Ensure 'State'/'Location' and other text columns are string dtype to safely use .str
    import pandas as _pd

    try:
        # Cast relevant columns to pandas "string" dtype if they exist
        for col in (
            "Location",
            "State",
            "Bedrooms",
            "Bathrooms",
            "Down_Payment",
            "Price",
            "Area",
        ):
            if col in df_clean.columns:
                df_clean[col] = df_clean[col].astype("string")

        # Ensure 'Location' exists
        if "Location" not in df_clean.columns:
            return df_clean

        # Split Location into parts and concatenate (drop original Location to avoid duplication)
        df_clean_split = (
            df_clean["Location"].str.split(",", expand=True).add_prefix("Location_")
        )
        df_clean = _pd.concat(
            [df_clean.drop(columns=["Location"]), df_clean_split], axis=1
        )

        # Drop Location_2 if present (some rows may not have 3 parts)
        if "Location_2" in df_clean.columns:
            df_clean = df_clean.drop(columns=["Location_2"])

        # Rename parts to meaningful names
        if "Location_1" in df_clean.columns:
            df_clean = df_clean.rename(columns={"Location_1": "State"})
        else:
            df_clean["State"] = _pd.NA

        if "Location_0" in df_clean.columns:
            df_clean = df_clean.rename(columns={"Location_0": "Location"})
        else:
            df_clean["Location"] = _pd.NA

        # Normalize text values safely on the df_clean dataframe
        if "State" in df_clean.columns:
            mask_state = df_clean["State"].notna()
            df_clean.loc[mask_state, "State"] = df_clean.loc[
                mask_state, "State"
            ].str.replace("Saba Pasha", "Saba Basha", case=False, regex=False)
            df_clean.loc[mask_state, "State"] = df_clean.loc[
                mask_state, "State"
            ].str.replace("Borg al-Arab", "Borg El Arab", case=False, regex=False)

        if "Location" in df_clean.columns:
            mask_loc = df_clean["Location"].notna()
            df_clean.loc[mask_loc, "Location"] = df_clean.loc[
                mask_loc, "Location"
            ].str.replace("Smoha", "Smouha", case=False, regex=False)

        # Fixing 'Price' column type casting error
        if "Price" in df_clean.columns:
            df_clean["Price"] = df_clean["Price"].str.replace(",", "").astype("int64")

        # Change column type to object for column: 'Area'
        if "Area" in df_clean.columns:
            df_clean["Area"] = df_clean["Area"].str.replace(",", "").astype("int")

        # Bedrooms/Bathrooms replacements (safe because cast to string dtype above)
        if "Bedrooms" in df_clean.columns:
            df_clean["Bedrooms"] = df_clean["Bedrooms"].str.replace(
                "+ Maid", " ", case=False, regex=False
            )
            df_clean["Bedrooms"] = df_clean["Bedrooms"].str.replace(
                "+", "", case=False, regex=False
            )
            df_clean["Bedrooms"] = df_clean["Bedrooms"].str.replace(
                "studio ", "1", case=False, regex=False
            )
            df_clean["Bedrooms"] = df_clean["Bedrooms"].str.replace(
                ".0", "", case=False, regex=False
            )
            df_clean = df_clean.astype({"Bedrooms": "int8"})

        if "Bathrooms" in df_clean.columns:
            df_clean["Bathrooms"] = df_clean["Bathrooms"].str.replace(
                "+", "", case=False, regex=False
            )
            df_clean["Bathrooms"] = df_clean["Bathrooms"].str.replace(
                ".0", "", case=False, regex=False
            )
            df_clean = df_clean.astype({"Bathrooms": "int8"})

        # Down_Payment cleaning
        if "Down_Payment" in df_clean.columns:
            df_clean["Down_Payment"] = df_clean["Down_Payment"].str.replace(
                " EGP", "", case=False, regex=False
            )
            df_clean["Down_Payment"] = df_clean["Down_Payment"].str.replace(
                "EGP", "", case=False, regex=False
            )
            df_clean["Down_Payment"] = df_clean["Down_Payment"].str.replace(
                "0% Down Payment", "0", case=False, regex=False
            )
            df_clean["Down_Payment"] = df_clean["Down_Payment"].str.replace(
                " 50 monthly / 1 year", "0", case=False, regex=False
            )
            df_clean["Down_Payment"] = df_clean["Down_Payment"].str.replace(
                "monthly / 1.5 years", "", case=False, regex=False
            )
            for years in range(1, 13):
                pattern = (
                    f"monthly / {years} years" if years > 1 else "monthly / 1 year"
                )
                df_clean["Down_Payment"] = df_clean["Down_Payment"].str.replace(
                    pattern, "", case=False, regex=False
                )
            df_clean["Down_Payment"] = df_clean["Down_Payment"].str.replace(",", "")
            df_clean["Down_Payment"] = df_clean["Down_Payment"].astype("int64")

        # Calculate Price_Per_M
        df_clean["Price_Per_M"] = df_clean["Price"] / df_clean["Area"]
        df_clean = df_clean.round({"Price_Per_M": 2})

        return df_clean

    except Exception as e:
        print(f"âŒ Error in clean_data_step2: {e}")
        import traceback

        traceback.print_exc()
        return df_clean


# ...existing code...


def process_and_save_data(df_raw, output_path):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    print("ğŸ”„ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")

    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù…Ù† Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    df_clean_1 = clean_data_step1(df_raw.copy())
    df1 = df_clean_1.copy()

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Down_Payment
    df1["Down_Payment"] = df1["Down_Payment"].fillna(0)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØµÙÙˆÙ Ø§Ù„ÙØ§Ø±ØºØ©
    initial_count = len(df1)
    df1.dropna(inplace=True)
    df1.reset_index(drop=True, inplace=True)
    print(f"âœ… ØªÙ…Øª Ø¥Ø²Ø§Ù„Ø© {initial_count - len(df1)} ØµÙ ÙØ§Ø±Øº")

    # ØªÙ†Ø¸ÙŠÙ Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„Ù…ÙˆÙ‚Ø¹
    df_clean = df1.copy()
    df_clean["State"] = df_clean["State"].str.strip()

    # ØªØµØ­ÙŠØ­ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù†Øª State ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "Alexandria"
    mask = df_clean["State"].str.contains("Alexandria", case=False, na=False)
    df_clean.loc[mask, "State"] = df_clean.loc[mask, "Location"]
    df_clean["State"] = df_clean["State"].fillna(df_clean["Location"])

    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ
    df_clean["State"] = df_clean["State"].str.strip()
    df_clean["Location"] = df_clean["Location"].str.strip()

    # Ø¥Ø¶Ø§ÙØ© Payment_Method
    df_clean["Payment_Method"] = ""
    df_clean.loc[
        df_clean["Down_Payment"].astype(str).str.strip() != "0", "Payment_Method"
    ] = "Installments"
    df_clean.loc[df_clean["Payment_Method"] == "", "Payment_Method"] = "Cash"

    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù…Ù† Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    df_clean = clean_data_step2(df_clean)

    # Ø¥Ø¶Ø§ÙØ© ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¬Ù…Ø¹
    # df_clean["Scraped_Date"] = datetime.now().strftime("%Y-%m-%d")

    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª
    if os.path.exists(output_path):
        try:
            df_final = pd.read_csv(output_path)
            print(f"ğŸ“ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø©: {len(df_final)} Ø¹Ù‚Ø§Ø±")

            # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            df_combined = pd.concat([df_final, df_clean], ignore_index=True)

            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            initial_combined = len(df_combined)
            df_combined.drop_duplicates(inplace=True)
            duplicates_removed = initial_combined - len(df_combined)

            if duplicates_removed > 0:
                print(f"ğŸ”„ ØªÙ…Øª Ø¥Ø²Ø§Ù„Ø© {duplicates_removed} Ø¹Ù‚Ø§Ø± Ù…ÙƒØ±Ø±")

            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            df_combined.to_csv(output_path, index=False)
            print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {len(df_combined)} Ø¹Ù‚Ø§Ø± ÙÙŠ {output_path}")
            print(
                f"ğŸ“ˆ Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„ØµØ§ÙÙŠ: {len(df_clean) - duplicates_removed:+d} Ø¹Ù‚Ø§Ø± Ø¬Ø¯ÙŠØ¯"
            )

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø©/Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©: {e}")
            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙ‚Ø·
            df_clean.to_csv(output_path, index=False)
            print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {len(df_clean)} Ø¹Ù‚Ø§Ø± ÙÙŠ {output_path} (Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯)")
    else:
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        df_clean.to_csv(output_path, index=False)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {len(df_clean)} Ø¹Ù‚Ø§Ø± ÙÙŠ {output_path} (Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯)")

    return df_clean


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ù…Ù† Bayut")
    print("=" * 50)
    print(f"ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    base_url = "https://www.bayut.eg/en/alexandria/properties-for-sale/"
    max_pages = 50  # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¬Ù…Ø¹Ù‡Ø§
    output_path = "Final1.csv"  # Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

    # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    print(f"\nğŸ“¥ Ø¬Ø§Ø±ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† {max_pages} ØµÙØ­Ø§Øª...")
    all_properties = scrape_all_pages(base_url, max_pages=max_pages)

    if not all_properties:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø¬Ù…Ø¹ Ø£ÙŠ Ø¹Ù‚Ø§Ø±Ø§Øª!")
        return False

    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ DataFrame
    df_raw = pd.DataFrame(all_properties)
    print(f"\nğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©: {len(df_raw)} Ø¹Ù‚Ø§Ø±")

    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df_final = process_and_save_data(df_raw, output_path)

    # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ
    print("\n" + "=" * 50)
    print("ğŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©:")
    print("=" * 50)
    print(f"Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©: {len(df_raw)}")
    print(f"Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df_final)}")
    print(f"Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø¹Ø±: {df_final['Price'].mean():,.0f} EGP")
    print(f"Ø·Ø±Ù‚ Ø§Ù„Ø¯ÙØ¹:")
    print(f"  - Ù†Ù‚Ø¯Ø§Ù‹: {(df_final['Payment_Method'] == 'Cash').sum()}")
    print(f"  - ØªÙ‚Ø³ÙŠØ·: {(df_final['Payment_Method'] == 'Installments').sum()}")
    print("=" * 50)
    print("âœ… Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¬Ù…Ø¹ Ø¨Ù†Ø¬Ø§Ø­!")

    return True


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
