# app.py - Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
import streamlit as st
import pandas as pd
import requests
import numpy as np
from bs4 import BeautifulSoup
import time
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import sqlite3
import json
import os

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Real Estate Scraper",
    page_icon="ğŸ ",
    layout="wide"
)

# ØªØ®ØµÙŠØµ Ø§Ù„ØªØµÙ…ÙŠÙ…
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 10px;
        text-align: center;
        margin: 5px;
    }
    .stButton>button {
        width: 100%;
    }
</style>
""", unsafe_allow_html=True)

# ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
def init_database():
    """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© SQLite Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    conn = sqlite3.connect('real_estate.db')
    cursor = conn.cursor()
    
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS properties (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        property_type TEXT,
        title TEXT,
        price TEXT,
        location TEXT,
        area TEXT,
        bedrooms TEXT,
        bathrooms TEXT,
        down_payment TEXT,
        payment_method TEXT,
        scraped_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        link TEXT UNIQUE
    )
    ''')
    
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS scrape_logs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        scrape_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        properties_count INTEGER,
        status TEXT
    )
    ''')
    
    conn.commit()
    return conn

# Ø¯Ø§Ù„Ø© Ù„Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
def scrape_bayut_page(page_url):
    """Ø¯Ø§Ù„Ø© Ù„Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø©"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: {e}")
        return []
    
    def text_or_none(selector, parent):
        el = parent.select_one(selector)
        return el.get_text(strip=True) if el else None
    
    property_cards = soup.select("ul._172b35d1 li")
    properties = []
    
    for card in property_cards:
        try:
            a = card.select_one("a._8969fafd")
            link = f"https://www.bayut.eg{a.get('href')}" if a and a.get('href') else None
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø± Ø§Ù„Ø±Ø§Ø¨Ø·
            if link:
                conn = init_database()
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM properties WHERE link = ?", (link,))
                if cursor.fetchone()[0] > 0:
                    continue
            
            price = text_or_none("h4.afdad5da._71366de7 span.eff033a6", card) or text_or_none("span.eff033a6", card)
            title = text_or_none("h2._34c51035", card)
            
            spans = card.select("span._3002c6fb")
            type_ = spans[0].get_text(strip=True) if len(spans) > 0 else None
            bedrooms = spans[1].get_text(strip=True) if len(spans) > 1 else None
            bathrooms = spans[2].get_text(strip=True) if len(spans) > 2 else None
            
            location = text_or_none("h3._51c6b1ca", card)
            d = text_or_none("span.fd7ade6e", card)
            
            area_raw = text_or_none("h4._60820635._07b5f28e", card) or text_or_none("h4", card)
            area = area_raw[:-6] if area_raw and len(area_raw) > 6 else area_raw
            
            properties.append({
                'property_type': type_,
                'link': link,
                'title': title,
                'price': price,
                'location': location,
                'area': area,
                'bedrooms': bedrooms,
                'bathrooms': bathrooms,
                'down_payment': d,
            })
        except Exception as e:
            continue
    
    return properties

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
def auto_scrape_if_needed():
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ØªØ­Ø¯ÙŠØ« Ù…Ø·Ù„ÙˆØ¨Ù‹Ø§ ÙˆØªÙ†ÙÙŠØ°Ù‡"""
    conn = init_database()
    cursor = conn.cursor()
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«
    cursor.execute("SELECT MAX(scrape_date) FROM scrape_logs WHERE status = 'success'")
    last_scrape = cursor.fetchone()[0]
    
    # Ø¥Ø°Ø§ Ù…Ø± Ø£ÙƒØ«Ø± Ù…Ù† 24 Ø³Ø§Ø¹Ø© Ù…Ù†Ø° Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«
    if last_scrape:
        last_date = datetime.strptime(last_scrape, '%Y-%m-%d %H:%M:%S')
        if datetime.now() - last_date < timedelta(hours=24):
            return False
    
    # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ø¯ÙŠØ«
    try:
        st.info("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        
        # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        base_url = "https://www.bayut.eg/en/alexandria/properties-for-sale/"
        properties = []
        
        for page_num in range(1, 3):  # ØµÙØ­ØªÙŠÙ† ÙÙ‚Ø· Ù„Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙŠÙˆÙ…ÙŠ
            if page_num == 1:
                page_url = base_url
            else:
                page_url = f"{base_url.rstrip('/')}/page-{page_num}/"
            
            page_properties = scrape_bayut_page(page_url)
            properties.extend(page_properties)
            time.sleep(1)
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        for prop in properties:
            try:
                cursor.execute('''
                INSERT OR IGNORE INTO properties 
                (property_type, title, price, location, area, bedrooms, bathrooms, down_payment, link)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    prop['property_type'],
                    prop['title'],
                    prop['price'],
                    prop['location'],
                    prop['area'],
                    prop['bedrooms'],
                    prop['bathrooms'],
                    prop['down_payment'],
                    prop['link']
                ))
            except:
                continue
        
        # ØªØ³Ø¬ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ø¯ÙŠØ«
        cursor.execute(
            "INSERT INTO scrape_logs (properties_count, status) VALUES (?, ?)",
            (len(properties), 'success')
        )
        
        conn.commit()
        conn.close()
        st.success(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« {len(properties)} Ø¹Ù‚Ø§Ø± Ø¨Ù†Ø¬Ø§Ø­")
        return True
        
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ: {e}")
        cursor.execute(
            "INSERT INTO scrape_logs (properties_count, status) VALUES (?, ?)",
            (0, 'failed')
        )
        conn.commit()
        conn.close()
        return False

# Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    st.markdown('<h1 class="main-header">ğŸ  Ø¹Ù‚Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ø³ÙƒÙ†Ø¯Ø±ÙŠØ© - ØªØ­Ø¯ÙŠØ« ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙŠÙˆÙ…ÙŠ</h1>', unsafe_allow_html=True)
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.image("https://cdn-icons-png.flaticon.com/512/3067/3067256.png", width=80)
        st.title("âš™ï¸ Ø§Ù„ØªØ­ÙƒÙ…")
        
        if st.button("ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¢Ù†", type="primary"):
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
                auto_scrape_if_needed()
                st.rerun()
        
        st.markdown("---")
        st.markdown("### ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
        
        conn = init_database()
        cursor = conn.cursor()
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        cursor.execute("SELECT COUNT(*) FROM properties")
        total_props = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(DISTINCT location) FROM properties")
        unique_locations = cursor.fetchone()[0]
        
        cursor.execute("SELECT MAX(scrape_date) FROM scrape_logs WHERE status = 'success'")
        last_update = cursor.fetchone()[0]
        
        st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª", total_props)
        st.metric("Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©", unique_locations)
        st.metric("Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«", last_update if last_update else "ØºÙŠØ± Ù…ØªØ§Ø­")
        
        conn.close()
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
    if 'auto_scraped' not in st.session_state:
        auto_scrape_if_needed()
        st.session_state.auto_scraped = True
    
    # Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    tab1, tab2, tab3 = st.tabs(["ğŸ  Ø¹Ø±Ø¶ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª", "ğŸ“ˆ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª", "âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"])
    
    with tab1:
        conn = init_database()
        
        # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØµÙÙŠØ©
        col1, col2, col3 = st.columns(3)
        
        with col1:
            locations_query = "SELECT DISTINCT location FROM properties WHERE location IS NOT NULL"
            locations = [row[0] for row in conn.execute(locations_query).fetchall()]
            selected_location = st.selectbox("Ø§Ù„Ù…Ù†Ø·Ù‚Ø©", ["Ø§Ù„ÙƒÙ„"] + locations)
        
        with col2:
            types_query = "SELECT DISTINCT property_type FROM properties WHERE property_type IS NOT NULL"
            types = [row[0] for row in conn.execute(types_query).fetchall()]
            selected_type = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±", ["Ø§Ù„ÙƒÙ„"] + types)
        
        with col3:
            sort_by = st.selectbox("ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨", ["Ø§Ù„Ø£Ø­Ø¯Ø«", "Ø§Ù„Ø³Ø¹Ø±", "Ø§Ù„Ù…Ù†Ø·Ù‚Ø©"])
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        query = "SELECT * FROM properties WHERE 1=1"
        params = []
        
        if selected_location != "Ø§Ù„ÙƒÙ„":
            query += " AND location = ?"
            params.append(selected_location)
        
        if selected_type != "Ø§Ù„ÙƒÙ„":
            query += " AND property_type = ?"
            params.append(selected_type)
        
        if sort_by == "Ø§Ù„Ø£Ø­Ø¯Ø«":
            query += " ORDER BY scraped_date DESC"
        elif sort_by == "Ø§Ù„Ø³Ø¹Ø±":
            query += " ORDER BY price DESC"
        elif sort_by == "Ø§Ù„Ù…Ù†Ø·Ù‚Ø©":
            query += " ORDER BY location"
        
        # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = pd.read_sql_query(query, conn, params=params)
        
        if not df.empty:
            st.dataframe(
                df[['title', 'price', 'location', 'property_type', 'bedrooms', 'bathrooms', 'area', 'scraped_date']],
                use_container_width=True,
                hide_index=True
            )
        else:
            st.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù‚Ø§Ø±Ø§Øª ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¨Ø­Ø«")
        
        conn.close()
    
    with tab2:
        st.markdown("### ğŸ“ˆ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©")
        
        conn = init_database()
        
        # Ø±Ø³ÙˆÙ…Ø§Øª Ø¨ÙŠØ§Ù†ÙŠØ©
        col1, col2 = st.columns(2)
        
        with col1:
            # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†Ø·Ù‚Ø©
            location_counts = pd.read_sql_query(
                "SELECT location, COUNT(*) as count FROM properties GROUP BY location ORDER BY count DESC LIMIT 10",
                conn
            )
            
            if not location_counts.empty:
                fig1 = px.bar(
                    location_counts,
                    x='location',
                    y='count',
                    title="Ø£ÙØ¶Ù„ 10 Ù…Ù†Ø§Ø·Ù‚ Ø¨Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª"
                )
                st.plotly_chart(fig1, use_container_width=True)
        
        with col2:
            # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            type_counts = pd.read_sql_query(
                "SELECT property_type, COUNT(*) as count FROM properties GROUP BY property_type",
                conn
            )
            
            if not type_counts.empty:
                fig2 = px.pie(
                    type_counts,
                    names='property_type',
                    values='count',
                    title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹"
                )
                st.plotly_chart(fig2, use_container_width=True)
        
        conn.close()
    
    with tab3:
        st.markdown("### âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ")
        
        st.info("""
        **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ:**
        - ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ ÙƒÙ„ 24 Ø³Ø§Ø¹Ø©
        - ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­Ø¯ÙŠØ« ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø²Ø± ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
        - ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª SQLite
        - Ø³Ø¬Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ù…Ø­ÙÙˆØ¸ Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª
        """)
        
        # Ø¹Ø±Ø¶ Ø³Ø¬Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª
        conn = init_database()
        logs = pd.read_sql_query(
            "SELECT * FROM scrape_logs ORDER BY scrape_date DESC LIMIT 10",
            conn
        )
        
        st.markdown("#### ğŸ“‹ Ø³Ø¬Ù„ Ø¢Ø®Ø± 10 ØªØ­Ø¯ÙŠØ«Ø§Øª")
        st.dataframe(logs, use_container_width=True)
        
        # Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
        with st.expander("Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"):
            if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"):
                conn.execute("DELETE FROM properties")
                conn.execute("DELETE FROM scrape_logs")
                conn.commit()
                st.success("ØªÙ… Ù…Ø³Ø­ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                st.rerun()
        
        conn.close()

if __name__ == "__main__":
    main()
